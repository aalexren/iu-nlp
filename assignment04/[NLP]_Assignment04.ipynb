{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Artem Chernitsa, B20-AI-01, a.chernitsa@innopolis.university"
      ],
      "metadata": {
        "id": "yvfUP-6MlNcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging\n",
        "\n",
        "During labs, we have covered HMM, LSTM and BERT models, the goal of this assignment is to evaluate and compare these models on POS tagging task. The input is a text line and the outputs are POS tags for every word (token) in the input line.\n",
        "\n",
        "- You should already have the code for the models from labs\n",
        "- Use validation split to decide when to stop training\n",
        "- Evaluate all models on test data\n",
        "- You can use any PoS tagging dataset to train and test your models\n",
        "\n",
        "Refer to:\n",
        "- Lab 4 - HMM for Tagging\n",
        "- Lab 10 - LSTM for Tagging\n",
        "- Lab 5 - Hugging Face and BERT fine-tuning\n",
        "- [Datasets](https://universaldependencies.org/)\n",
        "- [Dataset from Labs 4 and 10](https://raw.githubusercontent.com/Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt)\n",
        "\n",
        "\n",
        "Grading:\n",
        "- 30 points - HMM\n",
        "- 30 points - BiLSTM\n",
        "- 30 points - BERT (for masters only)\n",
        "- 40 points - Evaluation and conclusions \n",
        "\n",
        "\n",
        "Remarks: \n",
        "- Use Python 3\n",
        "- Max is 100 points for bachelors, 130 points for masters"
      ],
      "metadata": {
        "id": "3Z5Jca5Ah2b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR"
      ],
      "metadata": {
        "id": "vRFoyaVwfDLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Models for POS Tagging\n",
        "\n",
        "You can use the Viterbi algorithm implementation from Lab 4."
      ],
      "metadata": {
        "id": "7o7l16MVh2cH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preparing(data: str) -> tuple[dict, list, set, list]:\n",
        "    tokens: list[list[str]] = [line.split() for line in data.split(\"\\n\")]\n",
        "\n",
        "    tags: set[str] = set()\n",
        "    vocab: dict[str, int] = {}\n",
        "    sent_tags: list[str] = []\n",
        "    sents_tags: list[list[list[str]]] = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if len(token):\n",
        "            word = token[0].lower()\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "            sent_tags.append(token)\n",
        "\n",
        "            tags.add(token[1])\n",
        "        else:\n",
        "            if len(sent_tags):\n",
        "                sents_tags.append(sent_tags)\n",
        "            sent_tags = []\n",
        "\n",
        "    sents = [tuple(zip(*sent)) for sent in sents_tags]\n",
        "    \n",
        "    return vocab, sents, tags, sents_tags\n"
      ],
      "metadata": {
        "id": "eHX-LzX7ejp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_URL = \"https://raw.githubusercontent.com/\" \\\n",
        "    \"Gci04/AML-DS-2021/main/data/PosTagging/train_pos.txt\"\n",
        "TRAIN_DATA = requests.get(TRAIN_URL).text\n",
        "\n",
        "TEST_URL = \"https://raw.githubusercontent.com/\" \\\n",
        "    \"Gci04/AML-DS-2021/main/data/PosTagging/test_pos.txt\"\n",
        "TEST_DATA = requests.get(TEST_URL).text"
      ],
      "metadata": {
        "id": "tzE0VhUde11_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparing\n",
        "train_vocab, train_sents, train_tags, train_sents_tags = \\\n",
        "    data_preparing(TRAIN_DATA)\n",
        "\n",
        "test_vocab, test_sents, test_tags, test_sents_tags = \\\n",
        "    data_preparing(TEST_DATA)\n",
        "\n",
        "len(train_sents), len(test_sents)"
      ],
      "metadata": {
        "id": "H9lTpEgyh2cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc30041-62c1-4fa0-8ba5-3da7e19c451b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8936, 2012)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TAGS = train_tags | test_tags\n",
        "VOCAB = set((train_vocab | test_vocab).keys())\n",
        "\n",
        "TAGS_DICT = {k: v for v, k in enumerate(TAGS)}\n",
        "VOCAB_DICT = {k: v for v, k in enumerate(VOCAB)}"
      ],
      "metadata": {
        "id": "GR_7d2r2pZFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# viterbi\n",
        "def viterbi(y, A, B, Pi=None):\n",
        "    \"\"\"\n",
        "    Return the MAP estimate of state trajectory of Hidden Markov Model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : array (T,)\n",
        "        Observation state sequence. int dtype.\n",
        "    A : array (K, K)\n",
        "        State transition matrix. See HiddenMarkovModel.state_transition  for\n",
        "        details.\n",
        "    B : array (K, M)\n",
        "        Emission matrix. See HiddenMarkovModel.emission for details.\n",
        "    Pi: optional, (K,)\n",
        "        Initial state probabilities: Pi[i] is the probability x[0] == i. If\n",
        "        None, uniform initial distribution is assumed (Pi[:] == 1/K).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    x : array (T,)\n",
        "        Maximum a posteriori probability estimate of hidden state trajectory,\n",
        "        conditioned on observation sequence y under the model parameters A, B,\n",
        "        Pi.\n",
        "    T1: array (K, T)\n",
        "        the probability of the most likely path so far\n",
        "    T2: array (K, T)\n",
        "        the x_j-1 of the most likely path so far\n",
        "    \"\"\"\n",
        "    # Cardinality of the state space\n",
        "    K = A.shape[0]\n",
        "    # Initialize the priors with default (uniform dist) if not given by caller\n",
        "    Pi = Pi if Pi is not None else np.full(K, 1 / K)\n",
        "    T = len(y)\n",
        "    T1 = np.empty((K, T), 'd')\n",
        "    T2 = np.empty((K, T), 'B')\n",
        "\n",
        "    # Initilaize the tracking tables from first observation\n",
        "    T1[:, 0] = Pi * B[:, y[0]]\n",
        "    T2[:, 0] = 0\n",
        "\n",
        "    # Iterate throught the observations updating the tracking tables\n",
        "    for i in range(1, T):\n",
        "        T1[:, i] = np.max(T1[:, i - 1] * A.T * B[np.newaxis, :, y[i]].T, 1)\n",
        "        T2[:, i] = np.argmax(T1[:, i - 1] * A.T, 1)\n",
        "\n",
        "    # Build the output, optimal model trajectory\n",
        "    x = np.empty(T, 'B')\n",
        "    x[-1] = np.argmax(T1[:, T - 1])\n",
        "    for i in reversed(range(1, T)):\n",
        "        x[i - 1] = T2[x[i], i]\n",
        "\n",
        "    return x, T1, T2"
      ],
      "metadata": {
        "id": "BTaAHSrCf-8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viterbi_decode(tokens, A, B, Pi, vocab_dict, tags):\n",
        "    indexes = [vocab_dict.get(word.lower(), None) for word in tokens]\n",
        "    indexes = [i for i in indexes if i is not None]  # remove None values\n",
        "    if not indexes:\n",
        "        return None\n",
        "    x, _, _ = viterbi(np.array(indexes), A, B, Pi)\n",
        "    return [tags[tag_num] for tag_num in x if tag_num < len(tags)]"
      ],
      "metadata": {
        "id": "Cfndbke7UyQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# matrices"
      ],
      "metadata": {
        "id": "3Xinr_fdgEjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.zeros((44, 44))\n",
        "\n",
        "for sentence in train_sents_tags:\n",
        "    prev_tag = ''\n",
        "    for word, tag in sentence:\n",
        "        if prev_tag:\n",
        "            A[TAGS_DICT[prev_tag]][TAGS_DICT[tag]] += 1\n",
        "        prev_tag = tag\n",
        "\n",
        "A /= A.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(A[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTuzD9yIyJC9",
        "outputId": "cb8d23f1-0bb4-4456-beb6-8c01114b54a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.97464739e-02 5.27263940e-04 3.03176765e-03 6.21292675e-02\n",
            " 4.39386616e-04 1.88936245e-03 3.66887825e-02 1.80148513e-03\n",
            " 3.51509293e-04 1.44997583e-03 1.80148513e-03 4.39386616e-04\n",
            " 3.08010018e-02 3.26244563e-01 0.00000000e+00 2.77252955e-02\n",
            " 1.49830836e-02 0.00000000e+00 1.71360780e-03 1.36737115e-01\n",
            " 1.11560262e-01 1.31815985e-03 5.84384200e-03 0.00000000e+00\n",
            " 2.41662639e-03 0.00000000e+00 1.23028253e-03 4.39386616e-04\n",
            " 1.75754647e-04 2.78571115e-02 3.25146096e-03 7.90895909e-04\n",
            " 4.48174349e-03 0.00000000e+00 5.36051672e-03 1.75754647e-04\n",
            " 8.41864757e-02 4.83325278e-04 4.39386616e-05 2.19693308e-04\n",
            " 6.14701876e-02 3.42721561e-03 7.03018586e-04 6.06353530e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = np.zeros((44, len(VOCAB)))\n",
        "\n",
        "for sentence in train_sents_tags:\n",
        "    for word, tag in sentence:\n",
        "        B[TAGS_DICT[tag]][VOCAB_DICT[word.lower()]] += 1\n",
        "\n",
        "B /= B.sum(axis=1, keepdims=True)\n",
        "\n",
        "print(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNLQx7c3_tw6",
        "outputId": "1f72b419-6631-45a4-a18a-045116c8f0f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Pi = np.zeros(44)\n",
        "for sentence in train_sents_tags:\n",
        "    tag = sentence[0][1]\n",
        "    Pi[TAGS_DICT[tag]] += 1\n",
        "\n",
        "Pi /= (len(train_sents_tags))\n",
        "\n",
        "Pi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRh3Ewq6AMN6",
        "outputId": "735caea1-4577-4b3d-b2c6-582740e4c86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.28916741e-01, 4.47627574e-04, 4.92390331e-03, 1.62264996e-02,\n",
              "       3.13339302e-03, 2.23813787e-04, 8.61683080e-03, 5.61772605e-02,\n",
              "       3.35720680e-04, 3.24529991e-03, 0.00000000e+00, 4.81199642e-03,\n",
              "       5.83034915e-02, 2.12399284e-01, 0.00000000e+00, 1.34288272e-02,\n",
              "       6.03178156e-02, 0.00000000e+00, 6.37869293e-03, 1.91920322e-01,\n",
              "       4.52103850e-02, 1.90241719e-03, 5.25962399e-03, 6.71441361e-04,\n",
              "       3.24529991e-03, 1.11906893e-04, 0.00000000e+00, 2.12623098e-03,\n",
              "       1.11906893e-03, 6.71441361e-04, 7.83348254e-04, 0.00000000e+00,\n",
              "       0.00000000e+00, 4.47627574e-04, 1.11906893e-03, 5.59534467e-04,\n",
              "       4.48746643e-02, 2.12623098e-03, 1.11906893e-04, 0.00000000e+00,\n",
              "       4.36436885e-02, 3.35720680e-03, 1.11906893e-04, 7.27394808e-02])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM for POS Tagging\n",
        "\n",
        "Use a 2-layer BiLSTM from pytorch as we did in Lab 10\n",
        "- nn.LSTM(..., num_layers=2, bidirectional=True)"
      ],
      "metadata": {
        "id": "vLRfCS58h2cJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRvFGBnth2cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec7e344-e6c2-4388-9ad4-05d8399059a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(44, 19460)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# data preparing\n",
        "for tag in {\"<PAD>\", \"<UNK>\"}:\n",
        "    if tag not in TAGS_DICT:\n",
        "        TAGS_DICT[tag] = len(TAGS_DICT)\n",
        "    if tag not in VOCAB_DICT:\n",
        "        VOCAB_DICT[tag] = len(VOCAB_DICT)\n",
        "\n",
        "TAGS_DICT[\"<UNK>\"], VOCAB_DICT[\"<UNK>\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "def collate_fn(batch):\n",
        "    batch_input, batch_output = [], []\n",
        "\n",
        "    for sent in batch:\n",
        "        tokens_batch = [token.lower() for token in sent[0]]\n",
        "        tags_batch = sent[1]\n",
        "        \n",
        "        input_ = [\n",
        "            VOCAB_DICT.get(token, VOCAB_DICT['<UNK>']) \n",
        "            for token in tokens_batch[:20]\n",
        "        ]\n",
        "        output = [\n",
        "            TAGS_DICT.get(tag, TAGS_DICT['<UNK>']) \n",
        "            for tag in tags_batch[:20]\n",
        "        ]\n",
        "\n",
        "        input_ += [VOCAB_DICT['<PAD>']] * (20 - len(input_))\n",
        "        output += [TAGS_DICT['<PAD>']] * (20 - len(output))\n",
        "\n",
        "        batch_input.append(input_)\n",
        "        batch_output.append(output)\n",
        "\n",
        "    return \\\n",
        "        torch.tensor(batch_input, dtype=torch.int), \\\n",
        "        torch.tensor(batch_output, dtype=torch.int)"
      ],
      "metadata": {
        "id": "YzrEpLsWd2_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_RATIO = 0.1\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "stop_validation = int(len(train_sents) * VALIDATION_RATIO)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_sents[stop_validation:],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    train_sents[:stop_validation],\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_sents,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "IsrDX8BfIYCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, \n",
        "                 embed_dim, \n",
        "                 hidden_dim, \n",
        "                 num_layers, \n",
        "                 is_bidirectional, \n",
        "                 vocab_size, \n",
        "                 tagset_size):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            batch_first=True,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=is_bidirectional\n",
        "        )\n",
        "        lin_dim = (is_bidirectional + 1) * hidden_dim\n",
        "        self.hidden2tag = nn.Linear(lin_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sent):\n",
        "        embeds = self.word_embeds(sent)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "SLXsF2KsJQA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "VOCAB_SIZE = len(VOCAB_DICT)\n",
        "TARGET_SIZE = len(TAGS_DICT)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = LSTMTagger(\n",
        "    EMBEDDING_DIM,\n",
        "    HIDDEN_DIM,\n",
        "    NUM_LAYERS,\n",
        "    BIDIRECTIONAL,\n",
        "    VOCAB_SIZE,\n",
        "    TARGET_SIZE\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "ppjrrmqXfgRh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36bc9d7e-406a-49d6-97d7-daa69f0d90cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_calculator(preds, y):\n",
        "    return (preds == y).sum() / y.shape[0]\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    for text, tags in dataloader:\n",
        "        tags = tags.type(torch.LongTensor)\n",
        "        text = text.to(device)\n",
        "        tags = tags.to(device)\n",
        "\n",
        "        # initialize optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # predict tags\n",
        "        predictions = model(text)\n",
        "        predictions = predictions.view(-1, 46)\n",
        "        tags = tags.view(-1)\n",
        "        \n",
        "        # calculate loss\n",
        "        loss = criterion(predictions, tags)\n",
        "        acc = accuracy_calculator(torch.argmax(predictions.view(-1, 46), dim=1), tags)\n",
        "        \n",
        "        # backpropagate loss and optimize weights (2 lines)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item() * tags.shape[0]\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)"
      ],
      "metadata": {
        "id": "oFE4zGgzKSjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, data_batches, criterion, device):\n",
        "    eval_loss = 0\n",
        "    eval_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for text, tags in data_batches:\n",
        "\n",
        "            tags = tags.type(torch.LongTensor)\n",
        "\n",
        "            text = text.to(device)\n",
        "            tags = tags.to(device)\n",
        "\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, 46)\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            acc = accuracy_calculator(\n",
        "                torch.argmax(predictions.view(-1, 46), dim=1), tags\n",
        "            )\n",
        "\n",
        "            eval_loss += loss.item() * tags.shape[0]\n",
        "            eval_acc += acc.item()\n",
        "\n",
        "    return eval_loss / len(data_batches), eval_acc / len(data_batches)"
      ],
      "metadata": {
        "id": "hMZ3oFsoKWIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROGRESS_EPOCH = 5\n",
        "epochs = 100\n",
        "no_inc = 0\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    tr_loss, tr_acc = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    val_loss, val_acc = evaluate_model(model, validation_dataloader, criterion, device)\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "    print(f\"Train Loss:  {tr_loss:.3f}\\t\\tTrain Accuracy: {tr_acc*100:.2f}%\")\n",
        "    print(f\"Validation Loss:  {val_loss:.3f}\\tValidation Accuracy: {val_acc*100:.2f}%\\n\")\n",
        "\n",
        "    if val_acc > best_valid_acc:\n",
        "        best_valid_acc = val_acc\n",
        "        no_inc = 0\n",
        "    else:\n",
        "        no_inc += 1\n",
        "        if no_inc == PROGRESS_EPOCH:\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAK6Pt0EKsMv",
        "outputId": "56c55d08-a8cf-40ca-c7e4-974a1b16bc34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss:  4457.457\t\tTrain Accuracy: 54.44%\n",
            "Validation Loss:  1991.179\tValidation Accuracy: 76.93%\n",
            "\n",
            "Epoch: 2\n",
            "Train Loss:  1403.806\t\tTrain Accuracy: 83.79%\n",
            "Validation Loss:  1126.834\tValidation Accuracy: 86.75%\n",
            "\n",
            "Epoch: 3\n",
            "Train Loss:  787.530\t\tTrain Accuracy: 90.95%\n",
            "Validation Loss:  836.890\tValidation Accuracy: 90.25%\n",
            "\n",
            "Epoch: 4\n",
            "Train Loss:  450.680\t\tTrain Accuracy: 95.00%\n",
            "Validation Loss:  719.180\tValidation Accuracy: 91.60%\n",
            "\n",
            "Epoch: 5\n",
            "Train Loss:  244.373\t\tTrain Accuracy: 97.53%\n",
            "Validation Loss:  675.168\tValidation Accuracy: 92.47%\n",
            "\n",
            "Epoch: 6\n",
            "Train Loss:  124.109\t\tTrain Accuracy: 98.94%\n",
            "Validation Loss:  689.650\tValidation Accuracy: 92.77%\n",
            "\n",
            "Epoch: 7\n",
            "Train Loss:  60.558\t\tTrain Accuracy: 99.62%\n",
            "Validation Loss:  708.833\tValidation Accuracy: 93.07%\n",
            "\n",
            "Epoch: 8\n",
            "Train Loss:  29.924\t\tTrain Accuracy: 99.88%\n",
            "Validation Loss:  729.245\tValidation Accuracy: 93.26%\n",
            "\n",
            "Epoch: 9\n",
            "Train Loss:  16.474\t\tTrain Accuracy: 99.96%\n",
            "Validation Loss:  745.157\tValidation Accuracy: 93.41%\n",
            "\n",
            "Epoch: 10\n",
            "Train Loss:  10.503\t\tTrain Accuracy: 99.99%\n",
            "Validation Loss:  757.493\tValidation Accuracy: 93.40%\n",
            "\n",
            "Epoch: 11\n",
            "Train Loss:  7.308\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  771.881\tValidation Accuracy: 93.47%\n",
            "\n",
            "Epoch: 12\n",
            "Train Loss:  5.348\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  785.217\tValidation Accuracy: 93.47%\n",
            "\n",
            "Epoch: 13\n",
            "Train Loss:  4.198\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  795.056\tValidation Accuracy: 93.45%\n",
            "\n",
            "Epoch: 14\n",
            "Train Loss:  3.438\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  802.878\tValidation Accuracy: 93.48%\n",
            "\n",
            "Epoch: 15\n",
            "Train Loss:  2.885\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  813.936\tValidation Accuracy: 93.47%\n",
            "\n",
            "Epoch: 16\n",
            "Train Loss:  2.445\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  820.977\tValidation Accuracy: 93.52%\n",
            "\n",
            "Epoch: 17\n",
            "Train Loss:  2.106\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  828.954\tValidation Accuracy: 93.50%\n",
            "\n",
            "Epoch: 18\n",
            "Train Loss:  1.835\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  837.418\tValidation Accuracy: 93.52%\n",
            "\n",
            "Epoch: 19\n",
            "Train Loss:  1.618\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  842.843\tValidation Accuracy: 93.54%\n",
            "\n",
            "Epoch: 20\n",
            "Train Loss:  1.437\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  851.501\tValidation Accuracy: 93.54%\n",
            "\n",
            "Epoch: 21\n",
            "Train Loss:  1.282\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  856.772\tValidation Accuracy: 93.53%\n",
            "\n",
            "Epoch: 22\n",
            "Train Loss:  1.153\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  862.246\tValidation Accuracy: 93.54%\n",
            "\n",
            "Epoch: 23\n",
            "Train Loss:  1.042\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  869.452\tValidation Accuracy: 93.53%\n",
            "\n",
            "Epoch: 24\n",
            "Train Loss:  0.945\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  874.213\tValidation Accuracy: 93.54%\n",
            "\n",
            "Epoch: 25\n",
            "Train Loss:  0.861\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  878.047\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 26\n",
            "Train Loss:  0.788\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  885.360\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 27\n",
            "Train Loss:  0.723\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  889.920\tValidation Accuracy: 93.51%\n",
            "\n",
            "Epoch: 28\n",
            "Train Loss:  0.665\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  893.592\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 29\n",
            "Train Loss:  0.615\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  898.824\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 30\n",
            "Train Loss:  0.569\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  903.099\tValidation Accuracy: 93.56%\n",
            "\n",
            "Epoch: 31\n",
            "Train Loss:  0.528\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  908.035\tValidation Accuracy: 93.56%\n",
            "\n",
            "Epoch: 32\n",
            "Train Loss:  0.490\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  912.271\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 33\n",
            "Train Loss:  0.457\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  916.980\tValidation Accuracy: 93.57%\n",
            "\n",
            "Epoch: 34\n",
            "Train Loss:  0.426\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  920.078\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 35\n",
            "Train Loss:  0.399\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  924.868\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 36\n",
            "Train Loss:  0.373\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  928.557\tValidation Accuracy: 93.57%\n",
            "\n",
            "Epoch: 37\n",
            "Train Loss:  0.350\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  932.531\tValidation Accuracy: 93.58%\n",
            "\n",
            "Epoch: 38\n",
            "Train Loss:  0.329\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  936.438\tValidation Accuracy: 93.58%\n",
            "\n",
            "Epoch: 39\n",
            "Train Loss:  0.309\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  939.771\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 40\n",
            "Train Loss:  0.291\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  943.849\tValidation Accuracy: 93.56%\n",
            "\n",
            "Epoch: 41\n",
            "Train Loss:  0.274\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  947.416\tValidation Accuracy: 93.57%\n",
            "\n",
            "Epoch: 42\n",
            "Train Loss:  0.258\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  950.741\tValidation Accuracy: 93.58%\n",
            "\n",
            "Epoch: 43\n",
            "Train Loss:  0.244\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  953.520\tValidation Accuracy: 93.55%\n",
            "\n",
            "Epoch: 44\n",
            "Train Loss:  0.231\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  957.106\tValidation Accuracy: 93.60%\n",
            "\n",
            "Epoch: 45\n",
            "Train Loss:  0.218\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  960.720\tValidation Accuracy: 93.61%\n",
            "\n",
            "Epoch: 46\n",
            "Train Loss:  0.207\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  964.060\tValidation Accuracy: 93.58%\n",
            "\n",
            "Epoch: 47\n",
            "Train Loss:  0.196\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  967.311\tValidation Accuracy: 93.57%\n",
            "\n",
            "Epoch: 48\n",
            "Train Loss:  0.186\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  971.088\tValidation Accuracy: 93.58%\n",
            "\n",
            "Epoch: 49\n",
            "Train Loss:  0.176\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  973.365\tValidation Accuracy: 93.57%\n",
            "\n",
            "Epoch: 50\n",
            "Train Loss:  0.168\t\tTrain Accuracy: 100.00%\n",
            "Validation Loss:  976.979\tValidation Accuracy: 93.58%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT for POS Tagging\n",
        "\n",
        "You can fine-tune a pretrained model from HuggingFace or train a model from zero. You **don't** need to implement the model from scratch.\n",
        "\n",
        "Refer to the fine-tuning BERT part of Lab 5."
      ],
      "metadata": {
        "id": "-4KMf6Vid3gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparing"
      ],
      "metadata": {
        "id": "4Y1qMU6qh2cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "H-Yh_zLHfboq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "4pPb1QzzfZZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Kz6e0Irkd6z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate HMM\n",
        "def evaluate_hmm(test_words, test_tags):\n",
        "    correct, total= 0, 0\n",
        "    incorrect_ids = []\n",
        "\n",
        "    for i, test_sent in enumerate(test_words):\n",
        "        pred_tags = viterbi_decode(test_sent, A, B, Pi, VOCAB_DICT, list(TAGS))\n",
        "        correct_tags = test_tags[i]\n",
        "        \n",
        "        for j, tag in enumerate(pred_tags):\n",
        "            if tag == correct_tags[j]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        \n",
        "        if pred_tags != correct_tags:\n",
        "            incorrect_ids.append(i)\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    return accuracy, incorrect_ids"
      ],
      "metadata": {
        "id": "y1FkMd-fd_3K"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_word_list = []\n",
        "train_tag_list = []\n",
        "\n",
        "for sentence in train_sents_tags:\n",
        "    s_words, s_tags = zip(*sentence)\n",
        "    train_word_list.append(list(s_words))\n",
        "    train_tag_list.append(list(s_tags))"
      ],
      "metadata": {
        "id": "h7_Yx_HjaXD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_acc_train, hmm_ids_train = evaluate_hmm(train_word_list, train_tag_list)\n",
        "print(f'Accuracy on train data for HMM: {hmm_acc_train*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSpIvLVOaj81",
        "outputId": "bcbee5a7-5318-4b24-a5d8-fd83380fe374"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on train data for HMM: 97.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_word_list = []\n",
        "test_tag_list = []\n",
        "\n",
        "for sentence in test_sents_tags:\n",
        "    s_words, s_tags = zip(*sentence)\n",
        "    test_word_list.append(list(s_words))\n",
        "    test_tag_list.append(list(s_tags))"
      ],
      "metadata": {
        "id": "Xuhvw9E3WFpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hmm_acc_test, hmm_ids_test = evaluate_hmm(test_word_list, test_tag_list)\n",
        "print(f'Accuracy on test data for HMM: {hmm_acc_test*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBJmgihDZebM",
        "outputId": "192abc92-fd2d-485f-9da4-66b90cebfeae"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data for HMM: 51.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate LSTM\n",
        "test_loss, test_acc = evaluate_model(model, test_dataloader, criterion, device)\n",
        "print(f'Accuracy on test data for LSTM: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "suErLutGe5Ga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c554ae04-7088-4e75-9d1a-5a208b5ad251"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data for LSTM: 92.79%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate BERT"
      ],
      "metadata": {
        "id": "daqSd3oEe7uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print some samples where the models make mistakes\n",
        "print(\"HMM mistakes:\\n\")\n",
        "for idx in hmm_ids_test[:20]:\n",
        "    result = viterbi_decode(\n",
        "        test_word_list[idx], A, B, Pi, VOCAB_DICT, list(TAGS)\n",
        "    )\n",
        "    print(*zip(test_word_list[idx], result, test_tag_list[idx]))\n",
        "    print(\"======\")"
      ],
      "metadata": {
        "id": "KlA_k7Nee9P1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf25a9b-7308-4d87-9864-744cd518f4da"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMM mistakes:\n",
            "\n",
            "('Rockwell', 'IN', 'NNP') ('International', 'IN', 'NNP') ('Corp.', 'IN', 'NNP') (\"'s\", 'IN', 'POS') ('Tulsa', 'IN', 'NNP') ('unit', 'IN', 'NN') ('said', 'IN', 'VBD') ('it', 'IN', 'PRP') ('signed', 'IN', 'VBD') ('a', 'IN', 'DT') ('tentative', 'IN', 'JJ') ('agreement', 'IN', 'NN') ('extending', 'IN', 'VBG') ('its', 'IN', 'PRP$') ('contract', 'IN', 'NN') ('with', 'IN', 'IN') ('Boeing', 'IN', 'NNP') ('Co.', 'IN', 'NNP') ('to', 'IN', 'TO') ('provide', 'IN', 'VB') ('structural', 'IN', 'JJ') ('parts', 'IN', 'NNS') ('for', 'IN', 'IN') ('Boeing', 'IN', 'NNP') (\"'s\", 'IN', 'POS') ('747', 'IN', 'CD') ('jetliners', 'IN', 'NNS') ('.', 'IN', '.')\n",
            "======\n",
            "('Rockwell', 'IN', 'NNP') ('said', 'IN', 'VBD') ('the', 'IN', 'DT') ('agreement', 'IN', 'NN') ('calls', 'IN', 'VBZ') ('for', 'IN', 'IN') ('it', 'IN', 'PRP') ('to', 'IN', 'TO') ('supply', 'IN', 'VB') ('200', 'IN', 'CD') ('additional', 'IN', 'JJ') ('so-called', 'IN', 'JJ') ('shipsets', 'IN', 'NNS') ('for', 'IN', 'IN') ('the', 'IN', 'DT') ('planes', 'IN', 'NNS') ('.', 'IN', '.')\n",
            "======\n",
            "('These', 'DT', 'DT') ('include', 'VBP', 'VBP') (',', ',', ',') ('among', 'IN', 'IN') ('other', 'JJ', 'JJ') ('parts', 'NNS', 'NNS') (',', ',', ',') ('each', 'DT', 'DT') ('jetliner', 'NN', 'NN') (\"'s\", 'POS', 'POS') ('two', 'CD', 'CD') ('major', 'JJ', 'JJ') ('bulkheads', 'IN', 'NNS') (',', 'IN', ',') ('a', 'IN', 'DT') ('pressure', 'IN', 'NN') ('floor', 'IN', 'NN') (',', 'IN', ',') ('torque', 'IN', 'NN') ('box', 'IN', 'NN') (',', 'IN', ',') ('fixed', 'IN', 'VBN') ('leading', 'IN', 'VBG') ('edges', 'IN', 'NNS') ('for', 'IN', 'IN') ('the', 'IN', 'DT') ('wings', 'IN', 'NNS') ('and', 'IN', 'CC') ('an', 'IN', 'DT') ('aft', 'IN', 'JJ') ('keel', 'IN', 'NN') ('beam', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('Under', 'IN', 'IN') ('the', 'DT', 'DT') ('existing', 'VBG', 'VBG') ('contract', 'NN', 'NN') (',', ',', ',') ('Rockwell', 'IN', 'NNP') ('said', 'IN', 'VBD') (',', 'IN', ',') ('it', 'IN', 'PRP') ('has', 'IN', 'VBZ') ('already', 'IN', 'RB') ('delivered', 'IN', 'VBN') ('793', 'IN', 'CD') ('of', 'IN', 'IN') ('the', 'IN', 'DT') ('shipsets', 'IN', 'NNS') ('to', 'IN', 'TO') ('Boeing', 'IN', 'NNP') ('.', 'IN', '.')\n",
            "======\n",
            "('Rockwell', 'IN', 'NNP') (',', 'IN', ',') ('based', 'IN', 'VBN') ('in', 'IN', 'IN') ('El', 'IN', 'NNP') ('Segundo', 'IN', 'NNP') (',', 'IN', ',') ('Calif.', 'IN', 'NNP') (',', 'IN', ',') ('is', 'IN', 'VBZ') ('an', 'IN', 'DT') ('aerospace', 'IN', 'NN') (',', 'IN', ',') ('electronics', 'IN', 'NNS') (',', 'IN', ',') ('automotive', 'IN', 'JJ') ('and', 'IN', 'CC') ('graphics', 'IN', 'NNS') ('concern', 'IN', 'VBP') ('.', 'IN', '.')\n",
            "======\n",
            "('Frank', 'NNP', 'NNP') ('Carlucci', 'IN', 'NNP') ('III', 'IN', 'NNP') ('was', 'IN', 'VBD') ('named', 'IN', 'VBN') ('to', 'IN', 'TO') ('this', 'IN', 'DT') ('telecommunications', 'IN', 'NNS') ('company', 'IN', 'NN') (\"'s\", 'IN', 'POS') ('board', 'IN', 'NN') (',', 'IN', ',') ('filling', 'IN', 'VBG') ('the', 'IN', 'DT') ('vacancy', 'IN', 'NN') ('created', 'IN', 'VBN') ('by', 'IN', 'IN') ('the', 'IN', 'DT') ('death', 'IN', 'NN') ('of', 'IN', 'IN') ('William', 'IN', 'NNP') ('Sobey', 'IN', 'NNP') ('last', 'IN', 'JJ') ('May', 'IN', 'NNP') ('.', 'IN', '.')\n",
            "======\n",
            "('Mr.', 'NNP', 'NNP') ('Carlucci', 'IN', 'NNP') (',', 'IN', ',') ('59', 'IN', 'CD') ('years', 'IN', 'NNS') ('old', 'IN', 'JJ') (',', 'IN', ',') ('served', 'IN', 'VBN') ('as', 'IN', 'IN') ('defense', 'IN', 'NN') ('secretary', 'IN', 'NN') ('in', 'IN', 'IN') ('the', 'IN', 'DT') ('Reagan', 'IN', 'NNP') ('administration', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('In', 'IN', 'IN') ('January', 'NNP', 'NNP') (',', ',', ',') ('he', 'PRP', 'PRP') ('accepted', 'VBD', 'VBD') ('the', 'DT', 'DT') ('position', 'NN', 'NN') ('of', 'IN', 'IN') ('vice', 'NN', 'NN') ('chairman', 'NN', 'NN') ('of', 'IN', 'IN') ('Carlyle', 'IN', 'NNP') ('Group', 'IN', 'NNP') (',', 'IN', ',') ('a', 'IN', 'DT') ('merchant', 'IN', 'NN') ('banking', 'IN', 'NN') ('concern', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('Thomas', 'NNP', 'NNP') ('E.', 'NNP', 'NNP') ('Meador', 'IN', 'NNP') (',', 'IN', ',') ('42', 'IN', 'CD') ('years', 'IN', 'NNS') ('old', 'IN', 'JJ') (',', 'IN', ',') ('was', 'IN', 'VBD') ('named', 'IN', 'VBN') ('president', 'IN', 'NN') ('and', 'IN', 'CC') ('chief', 'IN', 'JJ') ('operating', 'IN', 'VBG') ('officer', 'IN', 'NN') ('of', 'IN', 'IN') ('Balcor', 'IN', 'NNP') ('Co.', 'IN', 'NNP') (',', 'IN', ',') ('a', 'IN', 'DT') ('Skokie', 'IN', 'NNP') (',', 'IN', ',') ('Ill.', 'IN', 'NNP') (',', 'IN', ',') ('subsidiary', 'IN', 'NN') ('of', 'IN', 'IN') ('this', 'IN', 'DT') ('New', 'IN', 'NNP') ('York', 'IN', 'NNP') ('investment', 'IN', 'NN') ('banking', 'IN', 'NN') ('firm', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('Balcor', 'IN', 'NNP') (',', 'IN', ',') ('which', 'IN', 'WDT') ('has', 'IN', 'VBZ') ('interests', 'IN', 'NNS') ('in', 'IN', 'IN') ('real', 'IN', 'JJ') ('estate', 'IN', 'NN') (',', 'IN', ',') ('said', 'IN', 'VBD') ('the', 'IN', 'DT') ('position', 'IN', 'NN') ('is', 'IN', 'VBZ') ('newly', 'IN', 'RB') ('created', 'IN', 'VBN') ('.', 'IN', '.')\n",
            "======\n",
            "('Mr.', 'NNP', 'NNP') ('Meador', 'IN', 'NNP') ('had', 'IN', 'VBD') ('been', 'IN', 'VBN') ('executive', 'IN', 'JJ') ('vice', 'IN', 'NN') ('president', 'IN', 'NN') ('of', 'IN', 'IN') ('Balcor', 'IN', 'NNP') ('.', 'IN', '.')\n",
            "======\n",
            "('In', 'IN', 'IN') ('addition', 'NN', 'NN') ('to', 'TO', 'TO') ('his', 'PRP$', 'PRP$') ('previous', 'JJ', 'JJ') ('real-estate', 'NN', 'NN') ('investment', 'NN', 'NN') ('and', 'CC', 'CC') ('asset-management', 'IN', 'NN') ('duties', 'IN', 'NNS') (',', 'IN', ',') ('Mr.', 'IN', 'NNP') ('Meador', 'IN', 'NNP') ('takes', 'IN', 'VBZ') ('responsibility', 'IN', 'NN') ('for', 'IN', 'IN') ('development', 'IN', 'NN') ('and', 'IN', 'CC') ('property', 'IN', 'NN') ('management', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('Those', 'DT', 'DT') ('duties', 'NNS', 'NNS') ('had', 'VBD', 'VBD') ('been', 'VBN', 'VBN') ('held', 'VBN', 'VBN') ('by', 'IN', 'IN') ('Van', 'NNP', 'NNP') ('Pell', 'IN', 'NNP') (',', 'IN', ',') ('44', 'IN', 'CD') (',', 'IN', ',') ('who', 'IN', 'WP') ('resigned', 'IN', 'VBD') ('as', 'IN', 'IN') ('an', 'IN', 'DT') ('executive', 'IN', 'JJ') ('vice', 'IN', 'NN') ('president', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('Shearson', 'NNP', 'NNP') ('is', 'VBZ', 'VBZ') ('about', 'RB', 'IN') ('60%-held', 'IN', 'JJ') ('by', 'IN', 'IN') ('American', 'IN', 'NNP') ('Express', 'IN', 'NNP') ('Co', 'IN', 'NNP') ('.', 'IN', '.')\n",
            "======\n",
            "('Great', 'NNP', 'NNP') ('American', 'NNP', 'NNP') ('Bank', 'NNP', 'NNP') (',', ',', ',') ('citing', 'VBG', 'VBG') ('depressed', 'JJ', 'JJ') ('Arizona', 'NNP', 'NNP') ('real', 'JJ', 'JJ') ('estate', 'NN', 'NN') ('prices', 'NNS', 'NNS') (',', ',', ',') ('posted', 'VBD', 'VBD') ('a', 'DT', 'DT') ('third-quarter', 'JJ', 'JJ') ('loss', 'NN', 'NN') ('of', 'IN', 'IN') ('$', 'IN', '$') ('59.4', 'IN', 'CD') ('million', 'IN', 'CD') (',', 'IN', ',') ('or', 'IN', 'CC') ('$', 'IN', '$') ('2.48', 'IN', 'CD') ('a', 'IN', 'DT') ('share', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('A', 'DT', 'DT') ('year', 'NN', 'NN') ('earlier', 'RBR', 'RBR') (',', ',', ',') ('the', 'DT', 'DT') ('savings', 'NNP', 'NNS') ('bank', 'NNP', 'VBP') ('had', 'VBD', 'VBD') ('earnings', 'NNS', 'NNS') ('of', 'IN', 'IN') ('$', '$', '$') ('8.1', 'CD', 'CD') ('million', 'CD', 'CD') (',', ',', ',') ('or', 'CC', 'CC') ('33', 'CD', 'CD') ('cents', 'NNS', 'NNS') ('a', 'DT', 'DT') ('share', 'NN', 'NN') ('.', '.', '.')\n",
            "======\n",
            "('For', 'IN', 'IN') ('the', 'DT', 'DT') ('nine', 'CD', 'CD') ('months', 'NNS', 'NNS') (',', ',', ',') ('it', 'PRP', 'PRP') ('had', 'VBD', 'VBD') ('a', 'DT', 'DT') ('loss', 'NN', 'NN') ('of', 'IN', 'IN') ('$', 'IN', '$') ('58.3', 'IN', 'CD') ('million', 'IN', 'CD') (',', 'IN', ',') ('or', 'IN', 'CC') ('$', 'IN', '$') ('2.44', 'IN', 'CD') ('a', 'IN', 'DT') ('share', 'IN', 'NN') (',', 'IN', ',') ('after', 'IN', 'IN') ('earnings', 'IN', 'NNS') ('of', 'IN', 'IN') ('$', 'IN', '$') ('29.5', 'IN', 'CD') ('million', 'IN', 'CD') (',', 'IN', ',') ('or', 'IN', 'CC') ('$', 'IN', '$') ('1.20', 'IN', 'CD') ('a', 'IN', 'DT') ('share', 'IN', 'NN') (',', 'IN', ',') ('in', 'IN', 'IN') ('the', 'IN', 'DT') ('1988', 'IN', 'CD') ('period', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('In', 'IN', 'IN') ('addition', 'NN', 'NN') ('to', 'TO', 'TO') ('the', 'DT', 'DT') ('increased', 'VBN', 'VBN') ('reserve', 'NN', 'NN') (',', ',', ',') ('the', 'DT', 'DT') ('savings', 'NNP', 'NNS') ('bank', 'NNP', 'VBP') ('took', 'VBD', 'VBD') ('a', 'DT', 'DT') ('special', 'JJ', 'JJ') ('charge', 'NN', 'NN') ('of', 'IN', 'IN') ('$', '$', '$') ('5', 'NN', 'NN') ('million', 'CD', 'CD') ('representing', 'VBG', 'VBG') ('general', 'JJ', 'JJ') ('and', 'CC', 'CC') ('administrative', 'JJ', 'JJ') ('expenses', 'NNS', 'NNS') ('from', 'IN', 'IN') ('staff', 'NN', 'NN') ('reductions', 'NNS', 'NNS') ('and', 'CC', 'CC') ('other', 'JJ', 'JJ') ('matters', 'NNS', 'NNS') (',', ',', ',') ('and', 'CC', 'CC') ('it', 'PRP', 'PRP') ('posted', 'VBD', 'VBD') ('a', 'DT', 'DT') ('$', '$', '$') ('7.6', 'CD', 'CD') ('million', 'CD', 'CD') ('reduction', 'NN', 'NN') ('in', 'IN', 'IN') ('expected', 'VBN', 'VBN') ('mortgage', 'NN', 'NN') ('servicing', 'IN', 'NN') ('fees', 'IN', 'NNS') (',', 'IN', ',') ('reflecting', 'IN', 'VBG') ('the', 'IN', 'DT') ('fact', 'IN', 'NN') ('that', 'IN', 'IN') ('more', 'IN', 'JJR') ('borrowers', 'IN', 'NNS') ('are', 'IN', 'VBP') ('prepaying', 'IN', 'VBG') ('their', 'IN', 'PRP$') ('mortgages', 'IN', 'NNS') ('.', 'IN', '.')\n",
            "======\n",
            "('Arbitragers', 'IN', 'NNS') ('were', 'IN', 'VBD') (\"n't\", 'IN', 'RB') ('the', 'IN', 'DT') ('only', 'IN', 'RB') ('big', 'IN', 'JJ') ('losers', 'IN', 'NNS') ('in', 'IN', 'IN') ('the', 'IN', 'DT') ('collapse', 'IN', 'NN') ('of', 'IN', 'IN') ('UAL', 'IN', 'NNP') ('Corp.', 'IN', 'NNP') ('stock', 'IN', 'NN') ('.', 'IN', '.')\n",
            "======\n",
            "('Look', 'NN', 'VB') ('at', 'IN', 'IN') ('what', 'WP', 'WP') ('happened', 'VBD', 'VBD') ('to', 'TO', 'TO') ('UAL', 'NNP', 'NNP') (\"'s\", 'POS', 'POS') ('chairman', 'NN', 'NN') (',', ',', ',') ('Stephen', 'NNP', 'NNP') ('M.', 'NNP', 'NNP') ('Wolf', 'NNP', 'NNP') (',', ',', ',') ('and', 'CC', 'CC') ('its', 'PRP$', 'PRP$') ('chief', 'JJ', 'JJ') ('financial', 'JJ', 'JJ') ('officer', 'NN', 'NN') (',', ',', ',') ('John', 'NNP', 'NNP') ('C.', 'NNP', 'NNP') ('Pope', 'NNP', 'NNP') ('.', '.', '.')\n",
            "======\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inv_vocab = {v: k for k, v in VOCAB_DICT.items()}\n",
        "inv_tags_vocab = {v: k for k, v in TAGS_DICT.items()}\n",
        "\n",
        "def show_model_issues(model, data_batches, device):\n",
        "    eval_acc = 0\n",
        "    count = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for text, tags in data_batches:\n",
        "            tags = tags.type(torch.LongTensor)\n",
        "            text = text.to(device)\n",
        "            tags = tags.to(device)\n",
        "            predictions = model(text)\n",
        "            predictions = predictions.view(-1, 46)\n",
        "            tags = tags.view(-1)\n",
        "            acc = accuracy_calculator(\n",
        "                torch.argmax(predictions.view(-1, 46), dim=1), tags\n",
        "            )\n",
        "            if acc < 1:\n",
        "                words = []\n",
        "                for p in text[0]:\n",
        "                    words.append(inv_vocab[p.item()])\n",
        "                ans_tags = torch.argmax(predictions.view(-1, 46), dim=1)\n",
        "                output = []\n",
        "                for idx in range(len(tags)):\n",
        "                    token = words[idx]\n",
        "                    pred_token = inv_tags_vocab[ans_tags[idx].item()]\n",
        "                    tag = inv_tags_vocab[tags[idx].item()]\n",
        "                    output.append((token, pred_token, tag))\n",
        "                print(*output)\n",
        "                print(\"======\")"
      ],
      "metadata": {
        "id": "GzM8Lixxgu61"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader_eval = DataLoader(\n",
        "    test_sents[:20],\n",
        "    batch_size = 1,\n",
        "    collate_fn=collate_fn)\n",
        "show_model_issues(model, test_dataloader_eval, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Cdpq66iKVS",
        "outputId": "8756ad62-5f39-4999-9f78-a45097189055"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('rockwell', 'NNP', 'NNP') ('international', 'NNP', 'NNP') ('corp.', 'NNP', 'NNP') (\"'s\", 'POS', 'POS') ('tulsa', 'JJ', 'NNP') ('unit', 'NN', 'NN') ('said', 'VBD', 'VBD') ('it', 'PRP', 'PRP') ('signed', 'VBD', 'VBD') ('a', 'DT', 'DT') ('tentative', 'JJ', 'JJ') ('agreement', 'NN', 'NN') ('extending', 'NN', 'VBG') ('its', 'PRP$', 'PRP$') ('contract', 'NN', 'NN') ('with', 'IN', 'IN') ('boeing', 'NNP', 'NNP') ('co.', 'NNP', 'NNP') ('to', 'TO', 'TO') ('provide', 'VB', 'VB')\n",
            "======\n",
            "('rockwell', 'NNP', 'NNP') ('said', 'VBD', 'VBD') ('the', 'DT', 'DT') ('agreement', 'NN', 'NN') ('calls', 'VBZ', 'VBZ') ('for', 'IN', 'IN') ('it', 'PRP', 'PRP') ('to', 'TO', 'TO') ('supply', 'VB', 'VB') ('200', 'CD', 'CD') ('additional', 'JJ', 'JJ') ('so-called', 'JJ', 'JJ') ('shipsets', 'NN', 'NNS') ('for', 'IN', 'IN') ('the', 'DT', 'DT') ('planes', 'NNS', 'NNS') ('.', '.', '.') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>')\n",
            "======\n",
            "('these', 'DT', 'DT') ('include', 'VBP', 'VBP') (',', ',', ',') ('among', 'IN', 'IN') ('other', 'JJ', 'JJ') ('parts', 'NNS', 'NNS') (',', ',', ',') ('each', 'DT', 'DT') ('jetliner', 'NN', 'NN') (\"'s\", 'POS', 'POS') ('two', 'CD', 'CD') ('major', 'JJ', 'JJ') ('bulkheads', 'NNS', 'NNS') (',', ',', ',') ('a', 'DT', 'DT') ('pressure', 'NN', 'NN') ('floor', 'NN', 'NN') (',', ',', ',') ('torque', 'RB', 'NN') ('box', 'NN', 'NN')\n",
            "======\n",
            "('under', 'IN', 'IN') ('the', 'DT', 'DT') ('existing', 'VBG', 'VBG') ('contract', 'NN', 'NN') (',', ',', ',') ('rockwell', 'NNP', 'NNP') ('said', 'VBD', 'VBD') (',', ',', ',') ('it', 'PRP', 'PRP') ('has', 'VBZ', 'VBZ') ('already', 'RB', 'RB') ('delivered', 'VBN', 'VBN') ('793', 'NN', 'CD') ('of', 'IN', 'IN') ('the', 'DT', 'DT') ('shipsets', 'NN', 'NNS') ('to', 'TO', 'TO') ('boeing', 'VB', 'NNP') ('.', '.', '.') ('<PAD>', '<PAD>', '<PAD>')\n",
            "======\n",
            "('rockwell', 'NNP', 'NNP') (',', ',', ',') ('based', 'VBN', 'VBN') ('in', 'IN', 'IN') ('el', 'NNP', 'NNP') ('segundo', 'NNP', 'NNP') (',', ',', ',') ('calif.', 'NNP', 'NNP') (',', ',', ',') ('is', 'VBZ', 'VBZ') ('an', 'DT', 'DT') ('aerospace', 'NN', 'NN') (',', ',', ',') ('electronics', 'NNP', 'NNS') (',', ',', ',') ('automotive', 'JJ', 'JJ') ('and', 'CC', 'CC') ('graphics', 'NNS', 'NNS') ('concern', 'NN', 'VBP') ('.', '.', '.')\n",
            "======\n",
            "('mr.', 'NNP', 'NNP') ('carlucci', 'NNP', 'NNP') (',', ',', ',') ('59', 'CD', 'CD') ('years', 'NNS', 'NNS') ('old', 'JJ', 'JJ') (',', ',', ',') ('served', 'VBD', 'VBN') ('as', 'IN', 'IN') ('defense', 'NNP', 'NN') ('secretary', 'NN', 'NN') ('in', 'IN', 'IN') ('the', 'DT', 'DT') ('reagan', 'NNP', 'NNP') ('administration', 'NN', 'NN') ('.', '.', '.') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>')\n",
            "======\n",
            "('shearson', 'NNP', 'NNP') ('is', 'VBZ', 'VBZ') ('about', 'IN', 'IN') ('60%-held', 'VBN', 'JJ') ('by', 'IN', 'IN') ('american', 'NNP', 'NNP') ('express', 'NNP', 'NNP') ('co', 'NNP', 'NNP') ('.', '.', '.') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>') ('<PAD>', '<PAD>', '<PAD>')\n",
            "======\n",
            "('a', 'DT', 'DT') ('year', 'NN', 'NN') ('earlier', 'RBR', 'RBR') (',', ',', ',') ('the', 'DT', 'DT') ('savings', 'NNS', 'NNS') ('bank', 'NN', 'VBP') ('had', 'VBD', 'VBD') ('earnings', 'NNS', 'NNS') ('of', 'IN', 'IN') ('$', '$', '$') ('8.1', 'CD', 'CD') ('million', 'CD', 'CD') (',', ',', ',') ('or', 'CC', 'CC') ('33', 'CD', 'CD') ('cents', 'NNS', 'NNS') ('a', 'DT', 'DT') ('share', 'NN', 'NN') ('.', '.', '.')\n",
            "======\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "95HCITItd9W5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your opinions and conclusions about the application of HMM, LSTM and BERT to PoS Tagging\n",
        "- discuss the results\n",
        "- pros and cons of each model\n",
        "- 4-6 sentences"
      ],
      "metadata": {
        "id": "JvfBnV93eAxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "The results of the evaluation reveal that LSTM model performs much better than HMM in PoS tagging task, with accuracy rates of 51.21% and 92.79%, respectively. The performance of LSTM is consistent across both validation and training sets, at around 93%. In contrast, HMM has a significantly high training set accuracy of 97.44%, but its generalization ability is lower than that of LSTM. One limitation of HMM is its **inability** to predict tags for unknown words, whereas LSTM can use **context** to make predictions. \n",
        "\n",
        "HMM has some advantages, such as **quick training** and the ability to perform well with a **large corpus**. However, it fails to capture context, which affects its generalization ability negatively. On the other hand, LSTM has a great **generalization ability**, which makes it more suitable for practical use. The context capturing of LSTM is limited when compared to BERT. Nonetheless, LSTM remains an effective model that performs well in PoS tagging tasks, but its training process **requiring more resources** than HMM.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UMZ1Rsvvh7dy"
      }
    }
  ]
}