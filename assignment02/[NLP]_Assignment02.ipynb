{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 40 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n",
        "- 20 points - Evaluate on Github Typo Corpus (for masters only)\n",
        "\n",
        "\n",
        "Remarks: \n",
        "- Use Python 3 or greater\n",
        "- Max is 80 points for bachelors, 100 points for masters"
      ],
      "metadata": {
        "id": "DIgM6C9HYUhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html).\n",
        "\n",
        "[N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n",
        "\n",
        "You may also wnat to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, Ukranian, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "Important - your project should not be a mere code copy-paste from somewhere. Implement yourself, analyze why it was suggested this way, and think of improvements/customization.\n",
        "\n",
        "Your solution should be able to perform 4 corrections per second (3-5 words in an example) on a typical cpu."
      ],
      "metadata": {
        "id": "x-vb8yFOGRDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import requests\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')\n",
        "\n",
        "import re\n",
        "import itertools\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Tuple, Set, Generator, Iterator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwU32BB6NCFj",
        "outputId": "d2af2ce1-5c95-46f5-b58c-9a4c26b86d91"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Corrector:\n",
        "    def __init__(self, ngram_dict_path: str):\n",
        "        self.model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        self.nd_path = ngram_dict_path # n-gram dictionary file path\n",
        "    \n",
        "    def load_nd(self):\n",
        "        \"Load n-gram dictionary to self.model from file\"\n",
        "        \n",
        "        with open(self.nd_path, \"r\") as f:\n",
        "            for s in f:\n",
        "                splitted = s.lower().split()\n",
        "                w_key, w_value = tuple(splitted[1:-1]), tuple(splitted[-1:])\n",
        "                count = int(splitted[0])\n",
        "                self.model[w_key][w_value] += count\n",
        "        \n",
        "        for w_key in self.model:\n",
        "            total_count = float(sum(self.model[w_key].values()))\n",
        "            for w_value in self.model[w_key]:\n",
        "                self.model[w_key][w_value] /= total_count"
      ],
      "metadata": {
        "id": "IVCoOjPEA9z-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NorvigCorrector(Corrector):\n",
        "    def __init__(self, datafile: str, ngram_dict_path: str=None):\n",
        "        super().__init__(ngram_dict_path)\n",
        "        self.words = Counter(self.tokens(open(datafile).read()))\n",
        "\n",
        "    def tokens(self, text: str) -> List:\n",
        "        return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def P(self, word: str, N=None) -> float:\n",
        "        \"Probability of `word`.\"\n",
        "        if not N:\n",
        "            N = sum(self.words.values())\n",
        "        return self.words[word] / N\n",
        "\n",
        "    def correction(self, word: str) -> str:\n",
        "        \"Most probable spelling correction for word.\"\n",
        "        return max(self.candidates(word), key=self.P)\n",
        "\n",
        "    def candidates(self, word: str) -> Set:\n",
        "        \"Generate possible spelling corrections for word.\"\n",
        "        return (self.known([word]) or\n",
        "                self.known(self.edits1(word)) or\n",
        "                self.known(self.edits2(word)) or\n",
        "                [word])\n",
        "\n",
        "    def known(self, words: Iterator) -> Set: \n",
        "        \"The subset of `words` that appear in the dictionary of self.words.\"\n",
        "        return set(w for w in words if w in self.words)\n",
        "\n",
        "    def edits1(self, word: str) -> Set:\n",
        "        \"All edits that are one edit away from `word`.\"\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word: str) -> Generator: \n",
        "        \"All edits that are two edits away from `word`.\"\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "\n",
        "    def correct(self, string: str) -> str:\n",
        "        \"\"\"\n",
        "        Return fixed string.\n",
        "        \"\"\"\n",
        "        tokens = re.findall(r\"\\w+\", string)\n",
        "        words = list(map(self.correction, [token.lower() for token in tokens]))\n",
        "        \n",
        "        str_arr = []\n",
        "        start_idx = 0\n",
        "        next_start_idx = 0\n",
        "        for idx, token in enumerate(tokens):\n",
        "            # find index of current token\n",
        "            next_start_idx = string.find(token, start_idx)\n",
        "            # add everything between tokens also (e.g. punctuation)\n",
        "            str_arr.append(string[start_idx:next_start_idx])\n",
        "            to_add = words[idx]\n",
        "            # keep word if uppercase\n",
        "            if token.istitle():\n",
        "                to_add = to_add.capitalize()\n",
        "            elif token.isupper():\n",
        "                to_add = to_add.upper()\n",
        "            str_arr.append(to_add)\n",
        "            # set start index to the next symbol after the word\n",
        "            start_idx = next_start_idx + len(token)\n",
        "        # add the rest after the last token\n",
        "        str_arr.append(string[start_idx:])\n",
        "\n",
        "        return ''.join(str_arr)\n",
        "    \n",
        "    def accuracy(self, \n",
        "                 source_sents: List[str],\n",
        "                 spoiled_sents: List[str]\n",
        "                 ) -> float:\n",
        "        \"\"\"\n",
        "        Count accuracy for the given list of strings.\n",
        "        \"\"\"\n",
        "        if len(source_sents) != len(spoiled_sents):\n",
        "            raise ValueError(\"Lists of different lengths!\")\n",
        "        \n",
        "        total_count = 0\n",
        "        total_errors = 0\n",
        "        for idx in range(len(source_sents)):\n",
        "            corrected_sent = self.correct(spoiled_sents[idx])\n",
        "            errors_count, tokens_count = self._error_counter(\n",
        "                source_sents[idx], corrected_sent\n",
        "                )\n",
        "            total_errors += errors_count\n",
        "            total_count += tokens_count\n",
        "        \n",
        "        return total_errors / total_count\n",
        "\n",
        "    \n",
        "    def _error_counter(self, source: str, corrected: str):\n",
        "        \"\"\"\n",
        "        Compares two strings by tokens and counts number of errors.\n",
        "        \"\"\"\n",
        "        source = re.findall(r\"\\w+\", source)\n",
        "        corrected = re.findall(r\"\\w+\", corrected)\n",
        "        if len(source) != len(corrected):\n",
        "            raise ValueError(\"String tokenized to different lengths.\")\n",
        "\n",
        "        return sum([s == c for s, c in zip(source, corrected)]), len(source)"
      ],
      "metadata": {
        "id": "cryYeW3e45tH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(\"https://norvig.com/big.txt\")\n",
        "big_data_path = \"big.txt\"\n",
        "with open(big_data_path, \"w\") as f:\n",
        "    f.write(r.text)"
      ],
      "metadata": {
        "id": "5cYUApTw7uu1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norvig = NorvigCorrector(big_data_path)\n",
        "print(norvig.candidates('fyck'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7YhIt1i8z60",
        "outputId": "f08d7cb1-a3de-4306-ce05-2a213c99475c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fuck'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r = requests.get(\"https://raw.githubusercontent.com/aalexren/iu-nlp/master/assignment02/w2_.txt\")\n",
        "bigrams_path = \"w2.txt\"\n",
        "with open(bigrams_path, \"w\") as f:\n",
        "    f.write(r.text)"
      ],
      "metadata": {
        "id": "5dZUA_SN-2wA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BlueberryCorrector(NorvigCorrector):\n",
        "    def __init__(self, datafile: str, ngram_dict_path: str):\n",
        "        super().__init__(datafile=datafile,\n",
        "                         ngram_dict_path=ngram_dict_path)\n",
        "\n",
        "    def correct(self, string: str) -> str:\n",
        "        \"\"\"\n",
        "        Return fixed string.\n",
        "        \"\"\"\n",
        "        tokens = re.findall(r\"\\w+\", string)\n",
        "        words = self._correct_spelling(string)\n",
        "        \n",
        "        str_arr = []\n",
        "        start_idx = 0\n",
        "        next_start_idx = 0\n",
        "        for idx, token in enumerate(tokens):\n",
        "            # find index of current token\n",
        "            next_start_idx = string.find(token, start_idx)\n",
        "            # add everything between tokens also (e.g. punctuation)\n",
        "            str_arr.append(string[start_idx:next_start_idx])\n",
        "            to_add = words[idx]\n",
        "            # keep word if uppercase\n",
        "            if token.istitle():\n",
        "                to_add = to_add.capitalize()\n",
        "            elif token.isupper():\n",
        "                to_add = to_add.upper()\n",
        "            str_arr.append(to_add)\n",
        "            # set start index to the next symbol after the word\n",
        "            start_idx = next_start_idx + len(token)\n",
        "        # add the rest after the last token\n",
        "        str_arr.append(string[start_idx:])\n",
        "\n",
        "        return ''.join(str_arr)\n",
        "\n",
        "    def _correct_spelling(self, string: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Correct spelling for tokens and returns list of them.\n",
        "        \"\"\"\n",
        "        words = self.tokens(string)\n",
        "        if len(words) == 1:\n",
        "            return words\n",
        "        \n",
        "        # ws_weights = [self.candidates(word) for word in words[:2]]\n",
        "        ws = [self.candidates(word) for word in words[:2]]\n",
        "        bigrams = itertools.product(*ws)\n",
        "        max_prob = max(bigrams, \n",
        "                        key=lambda x: self.model[x[:-1]][x[-1:]])\n",
        "        words[0] = max_prob[0]\n",
        "        for idx in range(1, len(words)):\n",
        "            bigram = words[idx - 1], words[idx]\n",
        "            ws = [self.candidates(word) for word in bigram]\n",
        "            bigrams = itertools.product(*ws)\n",
        "            max_prob = max(bigrams, \n",
        "                           key=lambda x: self.model[x[:-1]][x[-1:]])\n",
        "            if words[idx].isnumeric():\n",
        "                continue\n",
        "            words[idx] = max_prob[-1:][0]\n",
        "\n",
        "        return words"
      ],
      "metadata": {
        "id": "XhS3hI2V_VoH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blueberry = BlueberryCorrector(datafile=big_data_path,\n",
        "                               ngram_dict_path=bigrams_path)\n",
        "blueberry.load_nd()\n",
        "blueberry.correct(\"I lyke moreniing cofee\")\n",
        "import time\n",
        "t = time.time()\n",
        "print(blueberry.correct(\"Blck wman lks coffe.\"))\n",
        "print(blueberry.correct(\"Sugarr dady have a sex with madame.\"))\n",
        "print(blueberry.correct(\"Do u like semen?\"))\n",
        "print(time.time() - t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytpnu5btEEyW",
        "outputId": "5a26d2a1-f474-4893-f71d-d3b34fc47c3c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Black man los coffee.\n",
            "Sugar daddy have a sex with madame.\n",
            "Do u like semen?\n",
            "0.0019080638885498047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ErrorMaker:\n",
        "    def __init__(self, spoil_prob: float=0.5):\n",
        "        self.spoil_prob = spoil_prob # greater probability => more chances\n",
        "\n",
        "    def spoil(self, sentence: str) -> str:\n",
        "        tokens = re.findall(r\"\\w+\", sentence)\n",
        "        words = []\n",
        "        for token in tokens:\n",
        "            if random.random() < self.spoil_prob:\n",
        "                if random.random() < 0.95:\n",
        "                    words.append(\n",
        "                        random.choice(tuple(self._edits1(token.lower())))         \n",
        "                    )\n",
        "                else:\n",
        "                    words.append(\n",
        "                        random.choice(tuple(self._edits2(token.lower())))         \n",
        "                    )\n",
        "            else:\n",
        "                words.append(token)\n",
        "        \n",
        "        str_arr = []\n",
        "        start_idx = 0\n",
        "        next_start_idx = 0\n",
        "        for idx, token in enumerate(tokens):\n",
        "            # find index of current token\n",
        "            next_start_idx = sentence.find(token, start_idx)\n",
        "            # add everything between tokens also (e.g. punctuation)\n",
        "            str_arr.append(sentence[start_idx:next_start_idx])\n",
        "            to_add = words[idx]\n",
        "            # capitalize if it was titled\n",
        "            if token.istitle():\n",
        "                to_add = to_add.capitalize()\n",
        "            elif token.isupper():\n",
        "                to_add = to_add.upper()\n",
        "            str_arr.append(to_add)\n",
        "            # set start index to the next symbol after the word\n",
        "            start_idx = next_start_idx + len(token)\n",
        "        # add the rest after the last token\n",
        "        str_arr.append(sentence[start_idx:])\n",
        "\n",
        "        return ''.join(str_arr)\n",
        "\n",
        "    def _edits1(self, word: str) -> Set:\n",
        "        \"All edits that are one edit away from `word`.\"\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def _edits2(self, word: str) -> Set: \n",
        "        \"All edits that are two edits away from `word`.\"\n",
        "        return set(e2 for e1 in self._edits1(word) for e2 in self._edits1(e1))"
      ],
      "metadata": {
        "id": "NEL3pVbnI_Me"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_maker = ErrorMaker()\n",
        "err = error_maker.spoil(\"I like my mom!\")"
      ],
      "metadata": {
        "id": "k1xQ2wcWLNlp"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blueberry.correct(err)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n0QdIKVSMf4m",
        "outputId": "7ea94dd2-5024-4c6f-c9b8-7285594348bb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Id like he mob!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justify your decisions\n",
        "In your implementation you will need to decide which ngram dataset to use, which weights to assign for edit1, edit2 or absent words probabilities, beam search parameters and etc. Write down justificaitons for these choices."
      ],
      "metadata": {
        "id": "oML-5sJwGRLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, we had choice between bigrams and 5-grams dataset, cause there was a problem to obtain other. As for doing my own dataset I simply decided to use bigrams over the 5-grams, because it's not that sparse and could cover sentences of length two. Moreover, I used dictionary of words (unigrams) to cover single-word sentences.\n",
        "\n",
        "As for weights it could pe possible to change probabilities if we obtain it from `edit1` or `edit2` functions, hence we should keep from where do we get this result. I've decided instead just use different tecnhiques to generate errors, mostly for the my tests I set up probability of the first type of error to be generated 95% more than the second one. Theoretically we also could give more weight to choosing probability taking into account that for bigrams should give better result because of context, but actually let me give example: if the first token in sentence is error of `edits2`, then bigrams could spoiled entire sentence, because errors we be accumulated. Whereas errors of `edits1` more tolerant, and yet in case of huge number of errors unigrams could give better results for my case.\n",
        "\n",
        "Couple of words about absent words. If we couldn't find any possible unigram or bigram so we simply think it's just possible word, technically it's zero probability, but we don't mind, because we just yield it as it's the only option.\n",
        "\n",
        "Beam Search interesting thing as I found here: https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24, but I think it could also bumps into problem with accumulatig error, although to a lesser extent.\n",
        "\n",
        "\n",
        "Additionally, I noticed where is the problem with contractions e.g., it would be different tokens, and if unigrams of bigrams dataset doesn't have any occurence of it, they will lead to wrong correction of sentence. We could improve model by using using trigrams, 4-grams, 5-grams, beam search, but it also demands on the good dataset with small number of errors and semantically true dependencies."
      ],
      "metadata": {
        "id": "6Xb_twOmVsC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ],
      "metadata": {
        "id": "46rk65S4GRSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sents = nltk.sent_tokenize(nltk.corpus.treebank.raw()[:1000000])[:10]\n",
        "sents = [\n",
        "    \"I checked to make sure that he was still alive.\",\n",
        "    \"Many types occur just once in the LOB corpus.\",\n",
        "    \"Compact discs are available which contain a corpus of a whole century of poetry on a single disc.\",\n",
        "    \"Where could I find this fucking dataset with sentences?\",\n",
        "    \"I advise EVERYONE DO NOT BE FOOLED!\",\n",
        "    \"I bought this to use with my Kindle Fire and absolutely loved it!\",\n",
        "    \"Revolution in Just Listening is the third studio album to be released by Missouri band Coalesce, which was released on November 16, 1999 through Relapse Records.\",\n",
        "]\n",
        "\n",
        "spoileds = [error_maker.spoil(sent) for sent in sents]\n",
        "\n",
        "blu_acc = blueberry.accuracy(sents, spoileds)\n",
        "nor_acc = norvig.accuracy(sents, spoileds)\n",
        "\n",
        "print(f\"Blueberry's accuracy: {blu_acc:.2f}\")\n",
        "print(f\"Norvig's accuracy: {nor_acc:.2f}\")"
      ],
      "metadata": {
        "id": "OwZWaX9VVs7B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef8514e-e41f-4d78-b35b-ff79bf67c530"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blueberry's accuracy: 0.87\n",
            "Norvig's accuracy: 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"I did not have many expectations going into this with my daughter, I've seen the main Shrek movies but noon of the other Puss in Boot's spin-offs. I was shocked at how good a movie this was when the credits rolled!\n",
        "The story itself is functional, but all characters are given meaningful and relatable arcs. Between the melanchonic Puss reaching the end of his days as a great hero, his always positive dog sidekick and the 3 antagonistic parties trying to reach the magical macguffin there is a lot going on and none of it feels forced in.\n",
        "The two greatest assets the movie brings is it's humor and the stunning action scenes. First of all, this movie has bite, it is back to the tone of the first Shrek movie with lots of jokes that are working on a children and adult level of understanding. While the story is easy to follow for kids, the themes explored are relatively mature and will keep adults engaged.\n",
        "Secondly, while Disney/Pixar movies lately fail to make action exiting with their polished CGI style, Puss in boots goes full Into the Spiderverse once a fight breaks out. Glorious 12 frames per second, hyper stylized with all the filters and gimmicks necessary to elevate the big set-pieces to something truly special and memorable. Especially Puss's duels against a mysterious bounty hunter are the highlights.\n",
        "While not entirely original, the Puss in Boots: The Last Wish combines the edgy humor of Shrek with the visual wonders of Into the Spiderverse and strings it along a relatively matured heroes journey coming to it's end tale that is closer to Logan than any other animation I can think off. Oh and just in case (not that I personally care much about it) there is zero political agenda to be found here to be a distraction of the perception.\n",
        "\"\"\"\n",
        "sents = nltk.sent_tokenize(text)\n",
        "\n",
        "spoileds = [error_maker.spoil(sent) for sent in sents]\n",
        "print(spoileds)\n",
        "\n",
        "blu_acc = blueberry.accuracy(sents, spoileds)\n",
        "nor_acc = norvig.accuracy(sents, spoileds)\n",
        "\n",
        "print(f\"Blueberry's accuracy: {blu_acc:.2f}\")\n",
        "print(f\"Norvig's accuracy: {nor_acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBgr2jff2Mjd",
        "outputId": "52a65656-7afa-41df-dd5b-28f87986caca"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"I did noo have mdany expectatizons goinag into this hwith mym daughtter, Im've sveen tre maiq Shjek moaies but zoon of thej other Puss in Bnoot'g ssin-offs.\", 'Ni was shockedj wt how bood a movie this was wlen the credits roluled!', 'The story itsoelf ig functionbal, bqt alla characters qare mgixven meaningful and relatable arcs.', 'Between the melanchonic Push reaching the end dof hzuis days as ga gteat hero, hpis aliays positive dog sidekick ad thb j3 antagooistic parties trying to reach thve magical macguffin mthere is ac loh going on anvd nond gof it fmeels forceg hn.', \"Ieh twu greatoest assets the movie brings ias ivt'sb humor and thr stunning lction scenes.\", 'First ocf all, thms mogvie has wbite, it ia backn gto the tone of tme first Shrlk movie with lots of jokes that ane working an a children and adult ledvel fo underszanding.', 'Whilre mhe storky cis easy to follzow fsor kihds, she themes oxplored lre relatilvely mature aed will kaep adults engafed.', 'Secondly, while Disney/Pixjr mrovies lately fail to make action exitling with their polished CGI styla, Puss in boolts goes full Into uthe Shiderverse onci h fight ubreaks ouvt.', 'Glorious c12 fpames yper second, hyper stylized whith fall the filters atd grimmicks necessary zo elevate the kig set-piefces o somethqing trugly specialv and memouable.', \"Especially Cpuss'es duepls againsa a mysterious bounty hunter ase the highlights.\", \"While not entirely original, cthe Pugs in Btoots: The Lqst Wish cocmbines the ehgy humvor of Sprek wuith th visual wondefs ofn Into bhe Spiderverse and stgings it along i relazively mature herojes joaurney coming to it'c end talpe that is closer to Qognan than any otser animation I tan think ff.\", 'Oh and jusk in case (not that Ib personally caie much lbout kt) theore is zero politicoal agenda to he found here co be aq distraction of the perception.']\n",
            "Blueberry's accuracy: 0.81\n",
            "Norvig's accuracy: 0.80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"I did not expect the sequel to a decent spin-off Dreamworks film from over a decade ago to be one of the most poignant, introspective, genuinely hilarious, and heartwarming films of the year. But here we are.\n",
        "\n",
        "After an overly cheesy, somewhat clunky opening sequence, The Last Wish very quickly begins developing its zany assortment of characters into distinct quirky personalities with sympathetic desires and clear goals. The film juggles several character arcs and it's almost miraculous how it successfully handled all of them with proper set up and satisfying, emotionally weighty payoffs.\n",
        "\n",
        "The screenplay is wacky, witty, and also bursting with heart as it deals with weighty themes of trusting others and finding purpose in any circumstances. And it tackles these themes in ways that are always understandable to all ages but never insultingly oversimplified.\n",
        "\n",
        "What I also didn't expect was that the action sequences would be so well-choreographed and beautifully animated, and that the movie would often be terrifying and violent at times.\n",
        "\n",
        "I adored this film. I think it's Dreamworks' best film since Megamind and it's easily the best true family film of the year.\"\"\"\n",
        "\n",
        "sents = nltk.sent_tokenize(text)\n",
        "\n",
        "spoileds = [error_maker.spoil(sent) for sent in sents]\n",
        "print(spoileds)\n",
        "\n",
        "blu_acc = blueberry.accuracy(sents, spoileds)\n",
        "nor_acc = norvig.accuracy(sents, spoileds)\n",
        "\n",
        "print(f\"Blueberry's accuracy: {blu_acc:.2f}\")\n",
        "print(f\"Norvig's accuracy: {nor_acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK0XnxJt2_PT",
        "outputId": "d92d68fd-9512-408d-ec4d-8f0e39e41073"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fi dzid not exnect the sequeul to a decentn isppin-ofnf Dreamwrrks film from ovec va decade ago kto ba one of the most poignant, introspectivel, ogenuinely hilarious, and heartwarmiong films of theq year.', 'But here jrwe are.', 'After an overly cheesy, somewhat cluky openihng sequence, The Last Twish very quizckly begins developing its fzany assortment bf characters inio pistinct quirky persognalities witrh sympathetic desires and cpear goals.', \"Theg fhilm juggles severax chxaracter arcs and ix'j almost miralulous how it successfully hansled ayl of xthem with proper szet um and satisfying, emaotionally weightyu payoffs.\", 'Thme scrjenplay ies wacfky, wittcy, rnd alsuo bursting with hearp es itf deajls with weighty themes of trusting ethers abd finding purposen in any cirmcumstances.', 'Knd it tackles these themeks in ways that arde always understandable to all agbs bu nevev insultyngly oversimplified.', \"Uhat Zi also disdn'f expect was tthat the action sequences woumd ze so well-choreographed lnd beautifulny animated, ard that the movie would osten be tegrrifying rand violent at fimes.\", 'I adored twis firlmm.', \"Iw think it's Dreamworks' bestl film signce Megaminxd antd it'ds easily tie besn irue familb film of th yetar.\"]\n",
            "Blueberry's accuracy: 0.81\n",
            "Norvig's accuracy: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes better result keeps Blueberry, sometimes not, usually they differs for my implementation <5%, but it could be different, depends on sentences."
      ],
      "metadata": {
        "id": "rxxaw9Z2726Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Github Typo Corpus\n",
        "Now, you need to evaluate your solution on the Github Typo Corpus. Don't forget to compare the accuracy with the Norvig's solution."
      ],
      "metadata": {
        "id": "VISJtkFD4VhV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP9GZCYsWjgB"
      },
      "outputs": [],
      "source": [
        "!wget https://github-typo-corpus.s3.amazonaws.com/data/github-typo-corpus.v1.0.0.jsonl.gz\n",
        "!gzip -d github-typo-corpus.v1.0.0.jsonl.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "\n",
        "dataset_file = \"github-typo-corpus.v1.0.0.jsonl\"\n",
        "\n",
        "dataset = []\n",
        "other_langs = set()\n",
        "\n",
        "with jsonlines.open(dataset_file) as reader:\n",
        "    for obj in reader:\n",
        "        for edit in obj['edits']:\n",
        "            if edit['src']['lang'] == 'eng' and edit['is_typo']:\n",
        "                src, tgt = edit['src']['text'], edit['tgt']['text']\n",
        "                if src.lower() != tgt.lower():\n",
        "                    dataset.append((src, tgt))\n",
        "                \n",
        "print(f\"Dataset size = {len(dataset)}\")"
      ],
      "metadata": {
        "id": "oiUTzkLNGr2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please, explore the dataset. You may see, that this is\n",
        "- mostly markdown\n",
        "- some common mistakes with do/does\n",
        "- some just refer to punctuation typos (which we do not consider)"
      ],
      "metadata": {
        "id": "hjiKaUj0HKYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for pair in dataset[1010:1020]:\n",
        "    print(f\"{pair[0]} => {pair[1]}\")"
      ],
      "metadata": {
        "id": "gpkAqj6RHOr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare your implementation with Norvig's solution on this dataset"
      ],
      "metadata": {
        "id": "8p0kp4G-HexP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "limit = 10000\n",
        "counter = limit\n",
        "for i, (src, target) in enumerate(dataset):\n",
        "    if i == limit:\n",
        "        break\n",
        "    # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "vy85_6oKHE3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}